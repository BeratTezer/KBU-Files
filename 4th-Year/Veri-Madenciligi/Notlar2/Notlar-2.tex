\documentclass[12pt, a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage[turkish]{babel}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{geometry}
\geometry{a4paper, margin=1in}

\title{Çalışma Notları: Sınıflandırma: İleri Yöntemler (Bölüm 9)}
\date{\today}

\begin{document}
	
	\maketitle
	
	\section{Bölüm 9: Sınıflandırma: İleri Yöntemler (Devam)}
	
	\subsection{Tembel Öğreniciler (Lazy Learners)}
	
	\subsubsection{Örnek Tabanlı Akıl Yürütme (Case-Based Reasoning - CBR) (Sayfa 425)}
	CBR sınıflandırıcıları, yeni problemleri çözmek için bir \textbf{problem çözümleri veri tabanı} kullanır.
	\begin{itemize}
		\item \textbf{Temsil:} $k$-En Yakın Komşu yönteminin aksine, CBR, çoklukları veya "örnekleri" \textbf{karmaşık sembolik tanımlamalar} olarak depolar.
		\item \textbf{Uygulamalar:} Müşteri hizmetleri yardım masaları, mühendislik, hukuk ve tıbbi teşhis alanlarında kullanılır.
		\item \textbf{Zorluklar:} İyi bir \textbf{benzerlik metriği} bulmak ve çözümleri birleştirmek ana zorluklardır. Performansı artırmak için gereksiz örnekler atılabilir (editing method).
	\end{itemize}
	
	\subsection{Diğer Sınıflandırma Yöntemleri (Sayfa 426)}
	
	\subsubsection{Genetik Algoritmalar (Genetic Algorithms) (Sayfa 426)}
	\begin{itemize}
		\item Doğal seçilim ve genetik mekanizmalardan esinlenen bir \textbf{optimizasyon ve arama} tekniğidir.
		\item Sınıflandırma için \textbf{sınıflandırma kuralları} bulmak amacıyla kullanılır.
	\end{itemize}
	
	\subsubsection{Kaba Küme Yaklaşımı (Rough Set Approach) (Sayfa 427)}
	\begin{itemize}
		\item \textbf{Temel Fikir:} Verilen eğitim verileri içinde \textbf{denklik sınıfları} (equivalence classes) oluşturulmasına dayanır. Bir denklik sınıfındaki tüm çokluklar, tanımlayıcı öznitelikler açısından \textbf{ayırt edilemez} (indiscernible) durumdadır.
		\item \textbf{Yaklaşımlar:} Mevcut öznitelikler açısından tam olarak ayırt edilemeyen sınıfları \textbf{yaklaşık} olarak tanımlamak için \textbf{Alt Yaklaşım} (Lower Approximation) ve \textbf{Üst Yaklaşım} (Upper Approximation) kullanılır.
	\end{itemize}
	
	\subsubsection{Bulanık Küme Yaklaşımları (Fuzzy Set Approaches) (Sayfa 428)}
	\begin{itemize}
		\item \textbf{Avantajı:} \textbf{Belirsiz veya kesin olmayan olgularla} (vague or inexact facts) başa çıkmamızı sağlar.
		\item \textbf{Üyelik Fonksiyonu:} Bir eleman, farklı \textbf{üyelik dereceleri} (membership degrees) ile birden fazla bulanık kümeye ait olabilir. Geleneksel olasılık teorisinin aksine, üyelik değerlerinin toplamı 1 olmak zorunda değildir.
		\item \textbf{Kullanım Alanı:} Kural tabanlı sınıflandırma sistemlerinde, sürekli öznitelikler için \textbf{keskin sınırları} (sharp cut-offs) önlemede faydalıdır.
		\item \textbf{İşlemler:} Bulanık ölçümlerin birleştirilmesi için operasyonlar (Örneğin, $\text{AND} \rightarrow \text{minimum}$, $\text{OR} \rightarrow \text{maksimum}$) sağlar.
	\end{itemize}
	
	\subsection{Sınıflandırma ile İlgili Ek Konular (Sayfa 429 - 436)}
	
	\subsubsection{Çok Sınıflı Sınıflandırma (Multiclass Classification) (Sayfa 430)}
	İkiden fazla sınıfın olduğu durumlarda kullanılır (örn: SVM'ler doğal olarak ikili sınıflandırıcıdır).
	\begin{itemize}
		\item \textbf{Bire Karşı Hepsi (One-versus-All - OVA)}: $k$ sınıf için $k$ ikili sınıflandırıcı oluşturulur.
		\item \textbf{Hata Düzeltici Çıktı Kodları (Error-Correcting Output Codes - ECOC)}: Her bir sınıf, benzersiz bir $L$-bitlik \textbf{kod kelimesi} ile ilişkilendirilir. Sınıf ataması, çıktı kod kelimesi ile mevcut sınıf kod kelimeleri arasındaki \textbf{Hamming mesafesi} karşılaştırılarak yapılır.
	\end{itemize}
	
	\subsubsection{Yarı Denetimli Sınıflandırma (Semi-Supervised Classification) (Sayfa 432)}
	\textbf{Etiketli} ($X_l$) ve \textbf{etiketlenmemiş} ($X_u$) verilerin her ikisi de kullanılır.
	\begin{itemize}
		\item \textbf{Öz Eğitim (Self-Training)}: Sınıflandırıcı önce etiketli veriyle eğitilir, ardından en güvenilir etiketlenmemiş veriler etiketlenir ve eğitim setine eklenir (Hata takviyesi riski vardır).
		\item \textbf{Birlikte Eğitim (Co-Training)}: Özellikler iki karşılıklı dışlayıcı (mutually exclusive) alt kümeye ayrılır ve bu alt kümeler üzerinde iki ayrı sınıflandırıcı eğitilir.
	\end{itemize}
	
	\subsubsection{Aktif Öğrenme (Active Learning) (Sayfa 433)}
	Öğrenme algoritması, bilgi kazancını en üst düzeye çıkarmak için \textbf{etiketlenmemiş veri çokluklarından yalnızca birkaçını dikkatlice seçer} ve bir uzmandan yalnızca bunları etiketlemesini ister.
	\begin{itemize}
		\item \textbf{Belirsizlik Örneklemesi (Uncertainty Sampling)}: Algoritma, etiketleme konusunda en az emin olduğu çoklukları sorgular.
	\end{itemize}
	
	\subsubsection{Transfer Öğrenimi (Transfer Learning) (Sayfa 434)}
	Bir veya daha fazla \textbf{kaynak görevden} (source tasks) bilgi çıkarıp, bu bilgiyi farklı bir \textbf{hedef göreve} (target task) uygulamayı amaçlar.
	\begin{itemize}
		\item \textbf{Amaç:} Yeni bir modeli eğitmek için gereken etiketli veri miktarını azaltmaktır.
		\item \textbf{Yöntem:} \textbf{TrAdaBoost} gibi örnek tabanlı yaklaşımlar, kaynak verilerin ağırlıklarını otomatik olarak ayarlar ve hedef verilerden çok farklı olan eski verilerin etkisini filtreler.
		\item \textbf{Zorluk:} \textbf{Negatif transfer}, yani bilginin transferinin yeni sınıflandırıcının performansını düşürmesi riski mevcuttur.
	\end{itemize}
	
\end{document}